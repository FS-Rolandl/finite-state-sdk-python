#!/usr/bin/env python3

import argparse
import csv
import json
from datetime import datetime
from dotenv import load_dotenv
import os
import sys
from collections import defaultdict
import concurrent.futures
import time

# Add the parent directory to the Python path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import finite_state_sdk

def parse_args():
    parser = argparse.ArgumentParser(description='Report on vulnerability severity trends over time.')
    parser.add_argument('--secrets-file', help='Path to the secrets file (only required if .env not found in working directory)')
    parser.add_argument('--csv', nargs='?', const='vulnerability_severity_trends.csv', help='Export the report to a CSV file (default: vulnerability_severity_trends.csv)')
    parser.add_argument('--debug', action='store_true', help='Print debug information about the API response')
    parser.add_argument('--batch-size', type=int, default=10, help='Number of asset versions to process in each batch (default: 10)')
    return parser.parse_args()

def get_severity_score(severity):
    """Convert severity string to numeric score for comparison."""
    severity_map = {
        'CRITICAL': 4,
        'HIGH': 3,
        'MEDIUM': 2,
        'LOW': 1,
        'INFO': 0
    }
    return severity_map.get(severity.upper(), 0)

def print_progress(current, total, prefix=''):
    """Print a progress bar."""
    bar_length = 50
    filled_length = int(round(bar_length * current / float(total)))
    percents = round(100.0 * current / float(total), 1)
    bar = '=' * filled_length + '-' * (bar_length - filled_length)
    sys.stdout.write(f'\r{prefix}[{bar}] {percents}% ({current}/{total})')
    sys.stdout.flush()
    if current == total:
        sys.stdout.write('\n')

def process_batch(token, org_ctx, asset_versions, start_idx, batch_size, timeout_seconds=60):
    """Process a batch of asset versions and return their findings."""
    end_idx = min(start_idx + batch_size, len(asset_versions))
    batch = asset_versions[start_idx:end_idx]
    
    # Get findings for all versions in the batch
    findings_by_version = {}
    failed_versions = []
    
    for i, asset_version in enumerate(batch):
        version_id = asset_version.get('id')
        asset_name = asset_version.get('asset', {}).get('name', 'N/A')
        version_name = asset_version.get('name', 'N/A')
        
        # Print progress within batch
        sys.stdout.write(f'\rProcessing version {i+1}/{len(batch)}: {asset_name} ({version_name})...')
        sys.stdout.flush()
        
        def fetch_findings():
            return finite_state_sdk.get_findings(
                token,
                org_ctx,
                asset_version_id=version_id
            )
        
        try:
            print(f"\nFetching findings for {asset_name} ({version_name})...")
            start_time = time.time()
            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(fetch_findings)
                findings = future.result(timeout=timeout_seconds)
            elapsed = time.time() - start_time
            print(f"Processing findings for {asset_name} ({version_name})... (elapsed: {elapsed:.1f}s)")
            
            if isinstance(findings, list):
                findings_by_version[version_id] = findings
                sys.stdout.write(f' Found {len(findings)} findings\n')
            else:
                print(f"\nWarning: Unexpected findings format for {asset_name} ({version_name})")
                findings_by_version[version_id] = []
        except concurrent.futures.TimeoutError:
            print(f"\nTimeout: Fetching findings for {asset_name} ({version_name}) took longer than {timeout_seconds} seconds.")
            failed_versions.append({
                'id': version_id,
                'name': asset_name,
                'version': version_name
            })
            findings_by_version[version_id] = []
            continue
        except Exception as e:
            print(f"\nError fetching findings for {asset_name} ({version_name}): {str(e)}")
            print(f"Version ID: {version_id}")
            failed_versions.append({
                'id': version_id,
                'name': asset_name,
                'version': version_name,
                'error': str(e)
            })
            findings_by_version[version_id] = []
            continue  # Continue with next version even if this one fails
    
    return findings_by_version, failed_versions

def retry_failed_versions(token, org_ctx, failed_versions, timeout_seconds=120):
    """Retry fetching findings for failed versions with a longer timeout."""
    if not failed_versions:
        return {}
    
    print(f"\nRetrying {len(failed_versions)} failed versions with {timeout_seconds}s timeout...")
    findings_by_version = {}
    still_failed = []
    
    for i, version in enumerate(failed_versions):
        version_id = version['id']
        asset_name = version['name']
        version_name = version['version']
        
        print(f"\nRetry {i+1}/{len(failed_versions)}: {asset_name} ({version_name})...")
        
        def fetch_findings():
            return finite_state_sdk.get_findings(
                token,
                org_ctx,
                asset_version_id=version_id
            )
        
        try:
            start_time = time.time()
            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(fetch_findings)
                findings = future.result(timeout=timeout_seconds)
            elapsed = time.time() - start_time
            print(f"Success! Found {len(findings)} findings (elapsed: {elapsed:.1f}s)")
            findings_by_version[version_id] = findings
        except concurrent.futures.TimeoutError:
            print(f"Still failed: Timeout after {timeout_seconds}s")
            still_failed.append(version)
        except Exception as e:
            print(f"Still failed: {str(e)}")
            still_failed.append(version)
    
    if still_failed:
        print("\nVersions that still failed after retry:")
        for version in still_failed:
            print(f"- {version['name']} ({version['version']})")
    
    return findings_by_version

def main():
    args = parse_args()
    
    # Try to load .env from working directory first
    if not args.secrets_file and os.path.exists('.env'):
        load_dotenv('.env')
    elif args.secrets_file:
        load_dotenv(args.secrets_file)
    else:
        print("Error: No .env file found in working directory and no --secrets-file specified")
        sys.exit(1)

    # Get environment variables
    CLIENT_ID = os.environ.get("CLIENT_ID")
    CLIENT_SECRET = os.environ.get("CLIENT_SECRET")
    ORGANIZATION_CONTEXT = os.environ.get("ORGANIZATION_CONTEXT")

    if not all([CLIENT_ID, CLIENT_SECRET, ORGANIZATION_CONTEXT]):
        print("Error: Missing required environment variables (CLIENT_ID, CLIENT_SECRET, ORGANIZATION_CONTEXT)")
        sys.exit(1)

    # Get an auth token
    print("Getting auth token...")
    token = finite_state_sdk.get_auth_token(CLIENT_ID, CLIENT_SECRET)
    org_ctx = ORGANIZATION_CONTEXT

    # Get all asset versions
    print("\nFetching asset versions...")
    asset_versions = finite_state_sdk.get_asset_versions(token, org_ctx)
    total_versions = len(asset_versions)
    print(f"Found {total_versions} asset versions to analyze")
    
    # Prepare data structures for analysis
    asset_data = defaultdict(lambda: defaultdict(lambda: {
        'total_vulns': 0,
        'high_severity_vulns': 0,
        'created_at': None
    }))
    
    print("\nAnalyzing vulnerability severity trends...")
    batch_size = args.batch_size
    total_batches = (total_versions + batch_size - 1) // batch_size
    
    # Track all failed versions
    all_failed_versions = []
    
    # Process asset versions in batches
    for batch_num in range(total_batches):
        start_idx = batch_num * batch_size
        print(f"\nProcessing batch {batch_num + 1}/{total_batches} (versions {start_idx + 1}-{min(start_idx + batch_size, total_versions)})")
        
        try:
            # Get findings for this batch
            findings_by_version, failed_versions = process_batch(token, org_ctx, asset_versions, start_idx, batch_size)
            all_failed_versions.extend(failed_versions)
            
            # Process findings for each version in the batch
            for asset_version in asset_versions[start_idx:start_idx + batch_size]:
                asset = asset_version.get('asset', {})
                asset_name = asset.get('name', 'N/A')
                group_name = asset.get('group', {}).get('name', 'N/A')
                version_name = asset_version.get('name', 'N/A')
                created_at = asset_version.get('createdAt')
                
                # Get findings for this version
                findings = findings_by_version.get(asset_version.get('id'), [])
                
                # Count vulnerabilities by severity
                total_vulns = len(findings)
                high_severity_vulns = sum(1 for f in findings if get_severity_score(f.get('severity', '')) >= 3)
                
                # Store the data
                asset_data[asset_name][version_name].update({
                    'total_vulns': total_vulns,
                    'high_severity_vulns': high_severity_vulns,
                    'created_at': created_at,
                    'group': group_name
                })
        except Exception as e:
            print(f"\nError processing batch {batch_num + 1}: {str(e)}")
            print("Continuing with next batch...")
            continue
    
    # Retry failed versions with longer timeout
    if all_failed_versions:
        print(f"\nAttempting to retry {len(all_failed_versions)} failed versions with longer timeout...")
        retry_findings = retry_failed_versions(token, org_ctx, all_failed_versions)
        
        # Process any successful retries
        for version in all_failed_versions:
            version_id = version['id']
            if version_id in retry_findings:
                asset_name = version['name']
                version_name = version['version']
                findings = retry_findings[version_id]
                
                # Count vulnerabilities by severity
                total_vulns = len(findings)
                high_severity_vulns = sum(1 for f in findings if get_severity_score(f.get('severity', '')) >= 3)
                
                # Update the data
                if asset_name in asset_data and version_name in asset_data[asset_name]:
                    asset_data[asset_name][version_name].update({
                        'total_vulns': total_vulns,
                        'high_severity_vulns': high_severity_vulns
                    })
    
    # Prepare data for CSV export
    csv_data = []
    
    print("\nVulnerability Severity Trends:")
    print("-" * 120)
    print(f"{'Asset Name':<30} {'Group':<15} {'Version':<20} {'Created At':<20} {'Total Vulns':<12} {'High Severity':<12} {'High %':<8}")
    print("-" * 120)
    
    for asset_name, versions in asset_data.items():
        for version_name, data in versions.items():
            total_vulns = data['total_vulns']
            high_severity_vulns = data['high_severity_vulns']
            high_percentage = (high_severity_vulns / total_vulns * 100) if total_vulns > 0 else 0
            created_at = data['created_at']
            group = data['group']
            
            # Format the date
            if created_at:
                try:
                    date = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                    formatted_date = date.strftime('%Y-%m-%d')
                except:
                    formatted_date = created_at
            else:
                formatted_date = 'N/A'
            
            # Print to console
            print(f"{asset_name:<30} {group:<15} {version_name:<20} {formatted_date:<20} {total_vulns:<12} {high_severity_vulns:<12} {high_percentage:>6.1f}%")
            
            # Store for CSV
            csv_data.append({
                'Asset Name': asset_name,
                'Group': group,
                'Version': version_name,
                'Created At': formatted_date,
                'Total Vulnerabilities': total_vulns,
                'High Severity Vulnerabilities': high_severity_vulns,
                'High Severity Percentage': f"{high_percentage:.1f}%"
            })
    
    print("-" * 120)
    
    # Export to CSV if requested
    if args.csv:
        with open(args.csv, 'w', newline='') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=[
                'Asset Name', 'Group', 'Version', 'Created At',
                'Total Vulnerabilities', 'High Severity Vulnerabilities',
                'High Severity Percentage'
            ])
            writer.writeheader()
            writer.writerows(csv_data)
        print(f"\nReport exported to {args.csv}")

if __name__ == '__main__':
    main() 